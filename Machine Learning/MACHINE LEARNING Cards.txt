#separator:tab
#html:true
What is a Machine Learning?	ML is a component of AI, it is producing knowledge from data, learning a function f: X--&gt;Y
There are 3 main categories for ML problems:	"1. Supervised Learning
<br>&nbsp; &nbsp; &nbsp; &nbsp; Classification
<br>&nbsp; &nbsp; &nbsp; &nbsp; Regression
<br>2. Unsupervised Learning
<br>3. Reinforcement Learning"
What is a <b>Supervised Learning?</b>	"is a set of problems in which the dataset is formed by a
<br>set of pairs input-output. So, if I take an element from X I know the
<br>corresponding value in Y. So, X can be continuous or discrete, also Y.
<br>According to the different kind of set we have different names."
The difference between Classification and Regression in Supervised Learning	"Classification involves learning a function with a finite codomain, where the goal is to assign input instances to predefined classes. Regression, on the other hand, aims to approximate real-valued functions, where the output domain is infinite.<br><img src=""paste-330714172cbe28bdcdddec7e5c4ba7a3804c4dcc.jpg"">"
<div>Describe the nature of Unsupervised Learning.</div>	"<div><div><div><div><div><div><div><div><div><div><div><div><div>Unsupervised Learning involves only input values without corresponding labels in the codomain. Despite lacking explicit output labels, knowledge can still be extracted, such as identifying clusters, estimating density, or other statistical parameters. This type of analysis is often combined with classification tasks to enhance understanding.</div></div></div></div><div><div><div></div><div></div></div></div></div></div></div></div></div></div></div></div></div><div><div><div><img src=""paste-69a17b6937279bc630ace7612e97dd70671eacfb.jpg""><br></div></div></div>"
<div>Explain the concept of Reinforcement Learning.</div><br>	"Reinforcement Learning is employed in dynamic systems where the goal is to learn a function mapping a set of possible states �<span style=""font-style: italic;"">S</span> to actions and rewards, known as a transition policy. The data consists of sequences of actions and states, with associated values indicating the quality of each ""episode."" In essence, the agent learns to make optimal choices in varying situations over time.<img src=""paste-f7b0538de15f2eb2a3756a06e9c49dfd2de4a420.jpg"">"
Explain the concept of target function	"In machine learning, the target function c: X → Y&nbsp;represents the function we aim to learn, where <span style=""font-style: italic;"">X</span> is the instance space (input set) containing all possible inputs.<br>Each instance <span style=""font-style: italic;"">x</span>∈<span style=""font-style: italic;"">X</span> corresponds to a particular input."
explain dataset	[latex]The dataset \( D = \{(x_i, c(x_i))\} \) comprises pairs where each \( x_i \) is an instance from \( X \) and \( c(x_i) \) is the corresponding target function value, computable only for instances in the dataset.[/latex]
explain hypothesis space	"[latex]The hypothesis space \( H \) encompasses all possible functions that can be computed, with \( h \in H \) representing a hypothesis approximating the target function.
<br>[/latex]"
explain estimation in the context of machine learning	[latex]\( h(x) \) denotes the estimation of \( h \) over \( x \) (predicted value), and the objective is to ensure that \( h(x) \) closely resembles \( c(x) \), especially for values not in the dataset.[/latex]
"What defines consistency between a hypothesis <span style=""font-style: italic;"">h</span> and a training set <span style=""font-style: italic;"">D</span>?&nbsp;"	"<div>A hypothesis <span style=""font-style: italic;"">h</span> is considered consistent with a training set <span style=""font-style: italic;"">D</span> if it can perfectly approximate the function represented by the data points in <span style=""font-style: italic;"">D</span>.</div><br>"
"What is the ultimate goal in a learning task when a hypothesis space <span style=""font-style: italic;"">H</span> is involved?&nbsp;"	"<div><div><div><div><div><div><div><div><div><div><div><div><div>The ultimate goal is to find the best hypothesis <span style=""font-style: italic;"">h</span> within the hypothesis space <span style=""font-style: italic;"">H</span> that approximates the underlying function <span style=""font-style: italic;"">c</span> accurately, even for data points that are not in the training dataset <span style=""font-style: italic;"">D</span>.</div></div></div></div><div><div><div></div><div></div></div></div></div></div></div></div></div></div></div></div></div><div><div><div><br></div></div></div>"
Define an inductive learning hypothesis	[latex]An inductive learning hypothesis is characterized by the property that if a hypothesis $h$ performs well within the dataset $D$, accurately approximating $c(x_i)$ for all $x_i \in D$, then it is expected to perform well outside the dataset as well. However, for this generalization to hold, the dataset must be representative of the problem domain, ideally being large and diverse.<br>[/latex]
<div>What is a Version Space?</div><br>	<ul><li>The set of all hypotheses consistent with the observed training data, representing where the true concept is likely to lie in the hypothesis space.</li></ul><ul><li>The Version Space consists of hypotheses that are consistent with the dataset, mapping to subsets of instances consistent with the data.</li><li>If every subset of instances can be represented in a Version Space and all solutions are computed, the system is unable to classify new instances, as different hypotheses may yield different results.</li></ul><br>
What is the list-then-eliminate algorithm?	"We call list-then-eliminate algorithm an algorithm that compute the version
<br>space by iterating on all the hypothesis and checking for any if they are
<br>consistent. This in practice is not possible (we can have infinite hypothesis)."
What is a Noisy Data?	Collecting data is difficult and it’s easy to make some mistakes, these called<br><br>noisy data can be into the dataset and so the output is different from the true<br><br>value of the target function. In this case we can have no consistent hypothesis<br><br>so we need statistical methods to remove the noise.
Probability distribution of all possible inputs	[latex]To introduce the notion of performance metrics in ML, we define a probability distribution over X, denoted as Z. Let S be a set of n instances of X sampled with Z. Performance evaluations are based on accuracy and error rate.<br>[/latex]
True error of hypothesis h	"The true error of hypothesis h with respect to target function c and distribution Z is the probability that h will misclassify an instance according to Z.<img src=""paste-f35aacfd072237ab1c1290d9ed4972bc37af8003.jpg"">"
Sample error of hypothesis h	"The sample error of hypothesis h with respect to target function c and data sample S is the number of mistakes that h makes on S.<br><img src=""paste-e997e7800f726a4da79f414a195a74e6742cfc23.jpg"">"
<b>True error&nbsp;</b>vs&nbsp;<b>Sample error</b>	"True error represents a probability of making a mistake when the input is taken from&nbsp;<b>entire distribution</b>. True error can not be computed as we never know c(x).&nbsp;<br><br>Sample error is the number of mistakes h hypothesis makes on data sample (subset of D). It can be computed as we can know the values of the target function c(x) for the samples. The problem is that the true error of the whole distribution might be different.<br><br>Thus we can estimate true error as the goal is to be accurate in&nbsp;<img src=""paste-2b5c500d36ceed31e8890deff5bb80b173b1aa89.jpg"">"
What is Bias?	"The difference between the expected value of the extimator and the true value of the parameter<br><img src=""paste-445f0cd9f318f47e3c07e3d8291ea1204aebe168.jpg""><br>errorS(h) is an estimator of error D(h)"
Unbiased estimator	error<sub>D</sub>(h) is needed to evaluate hypothesis h, but cant be computed.<br>We must use an estimate.<br>The best we can do is to use error<sub>S</sub>(h) (suitably computed ==&gt; 0 bias)<br>Note that error<sub>D</sub>(h): true error<br>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; error<sub>S</sub>(h): Sample error<br><br>Unbiased Estimator:<br><ol><li>Partition data set, let the Training |T|=2/3|D|, who subsets are disjoint.</li><li>Compute a hypothesis h using training set T</li><li>Evaluate sample error&nbsp;[latex]\operatorname{errors}_S(h)=\frac{1}{n} \sum_{x \in S} \delta(f(x) \neq h(x))[/latex]</li></ol><br>
Confidence intervals	At the end of this we can make a statement, with some probability the true error is in an interval defined by the sample error, where these 2 values are defined by a coefficient z<sub>N</sub>&nbsp;.&nbsp;<br>[latex][ \operatorname{error}_S(h) \pm z_N \sqrt{\frac{\operatorname{error}_S(h)(1-\operatorname{error}_S(h))}{n}} ][/latex]
How to compare two hypothesis?	Given two hypothesis h<sub>1</sub>&nbsp;and h<sub>2</sub>&nbsp;<br>[latex]d \equiv \operatorname{error}_{\mathcal{D}}\left(h_1\right)-\operatorname{error}_{\mathcal{D}}\left(h_2\right)[/latex]<br>and its estimator is&nbsp;[latex]\hat{d} \equiv \operatorname{error}_{S_1}\left(h_1\right)-\operatorname{error}_{S_2}\left(h_2\right)[/latex]<br>&nbsp;<br>[latex]$\hat{d}$ is an unbiased estimator for $d$, iff $h_1, h_2, S_1$ and $S_2$ are independent from each other.[/latex]<br><br>Note that still valid if S<sub>1</sub>=S<sub>2</sub>=S
Explain overfitting	Hypothesis h overfits training data if there is an alternative hypothesis h' such that&nbsp;<br>[latex]\operatorname{errors}_S(h)&lt;\operatorname{error}_S\left(h^{\prime}\right)[/latex]<br>and&nbsp;<br>[latex]\operatorname{error}_{\mathcal{D}}(h)&gt;\operatorname{error}_{\mathcal{D}}\left(h^{\prime}\right)[/latex]<br><br>Meaning that sample error of h is lower, but true error is greater than h', so performs well locally, underperforms globally.&nbsp;
How can we evaluate the performance of a learning algorithm?	The performance of a learning algorithm can be evaluated using various metrics, including:<br><br>1. Accuracy: The proportion of correctly classified instances.<br><br>2. Precision: The proportion of true positive predictions among all positive predictions made by the model.<br><br>3. Recall: The proportion of true positive predictions among all actual positive instances in the data.<br><br>4. F1 Score: The harmonic mean of precision and recall, providing a balance between the two metrics.<br><br>5. Confusion Matrix: A table that summarizes the performance of a classification algorithm, showing true positive, true negative, false positive, and false negative predictions.<br><br>6. ROC Curve: A graphical representation of the trade-off between true positive rate and false positive rate at various classification thresholds.<br><br>7. Cross-Validation: A technique for estimating the performance of a model on unseen data by partitioning the dataset into training and validation sets multiple times.<br><br>These metrics help assess the accuracy, robustness, and generalization capability of the learning algorithm.
K-Fold Cross Validation	"<ul><li>the most commonly used method for evaluating classification problem.&nbsp;</li><li>evaluates learner.</li></ul><div>[latex]\begin{equation}</div>\begin{aligned}<br>&amp; D=S_1 \cup S_2 \cup \ldots \cup S_k \\<br>&amp; \text { for } i=1 \text { to } k \\<br>&amp; \quad \text { train } h_i \text { on } D \backslash S_i \\<br>&amp; \quad \delta=\delta+\text { error }_{S_i}\left(h_i\right) \\<br>&amp; \text { error }_{k, D}=\frac{\delta}{k}<br>\end{aligned}
<br><div>\end{equation}[/latex]<br></div><br>The error wil lbe the mean value of all errors computed.&nbsp;<br>Note: accuracy = 1 - error"
Present an example when Accuracy isnt a valid metric	let’s consider a binary classification problem with a dataset D with 98% of true and 2% of false.<br>A dumb hypothesis that returns always true has a very high accuracy. So we use other performance metrics
What are precision and recall?	Precision and recall are two important metrics used to evaluate the performance of classification algorithms, particularly in binary classification tasks.<br><br>Precision:<br>- Precision measures the proportion of true positive predictions among all positive predictions made by the model.<br>- It indicates the accuracy of positive predictions and is calculated as: <br>&nbsp; Precision = TP / (TP + FP), where TP is the number of true positive predictions and FP is the number of false positive predictions.<br><br>Recall:<br>- Recall (also known as sensitivity or true positive rate) measures the proportion of true positive predictions among all actual positive instances in the data.<br>- It indicates the ability of the model to identify all relevant instances and is calculated as: <br>&nbsp; Recall = TP / (TP + FN), where FN is the number of false negative predictions.<br><br>Precision and recall are complementary metrics: while precision focuses on the accuracy of positive predictions, recall focuses on the ability to capture all positive instances. Balancing these metrics is important depending on the specific requirements of the classification task.
What is F1-score?	The F1-score is a single metric that combines precision and recall into a single value, providing a more balanced measure of a model's performance in binary classification tasks.<br><br>Formula:<br>F1-score = 2 * (Precision * Recall) / (Precision + Recall)<br><br>- It takes into account both false positives (FP) and false negatives (FN) and is the harmonic mean of precision and recall.<br>- The F1-score ranges from 0 to 1, where a score of 1 indicates perfect precision and recall, and a score of 0 indicates either no precision or no recall.<br><br>The F1-score is useful when there is an uneven class distribution or when false positives and false negatives have different costs.
Explain Confusion Matrix	"represents in each entry how many instances of class C_i
<br>is misclassified as an element of class C_j . So, the main diagonal contains accuracy for each class.<br><img src=""paste-9c7f737dc4a00345f1bd9e2b3719565e17b85ca6.jpg"">"
Decision Trees in Hypothesis Space	In the context of machine learning, the problem revolves around finding a consistent hypothesis for a given training set D and target function c. One approach to solve this is by defining a hypothesis space H and implementing an algorithm to search for an h ∈ H that is consistent with D.<br><br>In our case, the hypothesis space consists of decision trees. Each hypothesis, or decision tree, is structured as follows:<br>- Each internal node represents an attribute.<br>- Each branch represents a value of an attribute.<br>- Each leaf node assigns a classification value.<br><br>Decision trees can be further interpreted as a set of rules, representing the disjunction of conjunctions of all paths leading to the positive leaf nodes. This allows for a clear and interpretable representation of the decision-making process.
ID3 Algorithm	"The ID3 (Iterative Dichotomiser 3) algorithm is a classic decision tree algorithm used for building decision trees from a training dataset. Here's how it works:<br><br>It uses&nbsp;<i>information gain</i>&nbsp;to measure how well a given attribute separates the training examples according to their target classification. Top node will be one with the most information gain.<br><br>Algorithm:&nbsp;<br><ol><li>create a root node for the tree</li><li>if all examples are positive, then return the node Root with label +</li><li>if all examples are negative, then return the node Root with label -</li><li>if Attributes is empty, then return node Root with label = most common value of Target_attrivute in Examples,&nbsp;</li><li><img src=""paste-4f1073a3fd7995d70e2870031d3cdcf4adc836a4.jpg""><br></li></ol>"
Issues with Decision Trees	<ul><li>Determining how deeply to grow the DT</li><li>Handling continuous attributes</li><li>Choosing appropriate attribute selection measures</li><li>Handling training data with missing attribute values</li><li>Handling attributes with different costs</li><li>Overfitting (but there are some ways to deal with it)</li></ul>
Ways to avoid/reduce overfitting in Decision Trees	"As DT tries to accomodate all instances, it tends to overfit.&nbsp;<br>We can:<br><ol><li>Stop growing when data split not statistically significant (before it overfits)</li><li>Let it overfit (grow full tree) then post-prune.</li></ol><div>To correctly determine the size of the tree:</div><div><ul><li>Use separate subset of examples (other then training set) ro evaluate the utility of post-pruning</li><li>apply a statistical test to estimate accuracy of a tree on the entire
<br>data distribution<br></li><li>using an explicit measure of the complexity for encoding the examples
<br>and the decision trees.<br></li></ul></div>"
What is rule post-pruning and reduced-error pruning	"reduced-post pruning:&nbsp;<br><img src=""paste-5076efc6dd5c8d5d214b00fcad3013e2fe2e32c7.jpg""><br>Rule post-pruning<br><img src=""paste-f1d3f6765077def134b08e34131fedbceb1ef9fe.jpg"">"
Random Forest	"method based on decision trees.<br>generates a set of decision trees using random criteria (e.g.
<br>random picking attributes) and integrates their values into a final value. Usually they perform better that one single decision tree. The result of the Random Forest is the most common classification of all the trees (majority vote) and they are less sensitive to overfitting."
"<span class=""cloze"" data-cloze=""Conditional&#x20;probability"" data-ordinal=""1"">[...]</span> is the probability of an event given another event happened"	"<span class=""cloze"" data-ordinal=""1"">Conditional probability</span> is the probability of an event given another event happened<br>
"
"<span class=""cloze"" data-cloze=""Prior&#x20;probability"" data-ordinal=""1"">[...]</span> is the probability of an event without knowing any other information (e.g. P(Odd = true) = 0.5 )."	"<span class=""cloze"" data-ordinal=""1"">Prior probability</span> is the probability of an event without knowing any other information (e.g. P(Odd = true) = 0.5 ).<br>
"
How do we interpret classification as a probabilistic estimation?<br>	"Given&nbsp;[latex]x' \notin D [/latex] the best prediction is&nbsp;[latex]h*(x')=y*[/latex]<br>[latex]v^*=\arg \max _{y \in Y} P\left(y \mid x^{\prime}, D\right)[/latex]<br><br>Where D is the dataset, and argmax returns the value of y for which the
<br>function is maximum (instead max{f(x)} returns the max value of f(x)). So, what we are going to compute is the probability distribution over Y (codomain):<img src=""paste-7bf390ee556d24ac454e10905c3a36bed40be2a5.jpg"">"
What is conditional independence	Conditional independence is a concept in probability theory that describes the relationship between random variables given the value of another variable. Two random variables X and Y are conditionally independent given a third variable Z if the probability distribution of X is independent of the value of Y when Z is known.<br><br>Formally, X is independent from Y given Z if and only if P(X, Y | Z) = P(X | Z)&nbsp; for all possible values of X, Y, and Z.<br><br>Conditional independence is a useful concept in various areas, including machine learning, where it helps simplify probabilistic models and inference algorithms.
Learning as a Probabilistic Estimation	In machine learning, learning as a probabilistic estimation refers to the approach of modeling learning problems using probabilistic frameworks. Instead of deterministic rules or algorithms, probabilistic methods represent uncertainty by assigning probabilities to different outcomes.<br><br>This approach allows machine learning models to make predictions or decisions while acknowledging uncertainty and variability in the data. It also provides a principled way to handle noisy or incomplete information.<br><br>Probabilistic estimation involves:<br>- Defining a probabilistic model that captures the relationship between inputs and outputs.<br>- Learning model parameters from training data using probabilistic inference techniques.<br>- Making predictions or decisions by computing probabilities or distributions over possible outcomes.<br><br>Common techniques for probabilistic estimation include Bayesian inference, maximum likelihood estimation, and probabilistic graphical models.
What is a bayes rule? Explain the formula	"\[\text{1. Maximum a posteriori (MAP) hypotheses:} \\
\text{MAP hypotheses are the hypotheses that maximize the posterior probability } P(h|D). \\
\text{In other words, given the observed data } D, \text{ MAP hypotheses are those that are most likely to be true.} \\\]<br>\[\text{2. Maximum likelihood (ML) hypotheses:} \\
\text{ML hypotheses are the hypotheses that maximize the likelihood } P(D|h) \text{ of the observed data } D \\
\text{given the hypotheses } h. \text{ In other words, ML hypotheses are those that make the observed data most probable under the given hypothesis.}\]<br>\[\text{These concepts are fundamental in Bayesian learning, where we seek to infer the most likely hypotheses given the observed data.} \\
\text{The MAP and ML hypotheses provide different perspectives on how we can approach this inference process.}\]<br><br><img src=""paste-28dab7aba8be091565a86b3242268cb8deacdf03.jpg"">"
Represent most probable hypothesis given D	"Maximum a posteriori hypothesis (MAP) called&nbsp;[latex]h<sub>MAP</sub>[/latex]<br><img src=""paste-225005675b30772839e3e0704e3d62e3f9ac3087.jpg"" width=""552""><br>Is defined as the argmax over all possible hypothesis in
<br>the hypothesis space, so it’s the value of h for which the probability is maximized. We have eliminated the probability of D ( P(D) ) because it doesn’t depend on h."
What is Maximum Likelihood (ML) hypothesis and how is it different from Maximum A Posteriori (MAP) hypothesis	"If we assume that all the hypotheses have the same probability we can simplify and choose the Maximum likelihood (ML) hypothesis:<br><img src=""paste-1edae15c2844acc510824e96587830e5f381cf32.jpg""><br>where MAP is&nbsp;<br><img src=""paste-5b0c3fe7342b7b4440c8eac260164714c397d030.jpg"">"
In general, we want the most probable hypothesis given D, this is called&nbsp;<b>maximum a posteriori hypothesis (MAP)&nbsp;</b>called h<sub>MAP.&nbsp;</sub>Explain	"<img src=""paste-5c26baaecd0e6aa0bd5d13b0c591c00d3ad1c580.jpg""><br>Is defined as the argmax over all possible hypothesis in
the hypothesis space, so it’s the value of h for which the probability is&nbsp;maximized. We have eliminated the probability of D ( P(D) ) because it doesn’t
depend on h.
If we assume that all the hypotheses have the same probability we can simplify
and choose the Maximum likelihood (ML) hypothesis:<br><img src=""paste-af7e77c8bc40a0deba889a50325d5a92bb893b7d.jpg"">"
Bayes Optimal Classifier	"If h<sub>MAP</sub> is the most probable hypothesis given dataset D, given a new instance
x’ ∉ D, hMAP(x’) may not be the most probable classification. We need to
take a weighted average, and then return as value the class for which you have
the highest weighted average.
Here we need to introduce the Bayes Optimal Classifier:<br><img src=""paste-259706e04c7c007da12735a05feb9381c85a0682.jpg""><br>It is optimal, but not computationally feasable solution"
Naive Bayes Classifier	"Uses&nbsp;<b>conditional independence&nbsp;</b>to approximate optimal bayes classifer. Two events are conditionally independent if:&nbsp;<br><img src=""paste-1530437e73428148bc6aa7879182e4cf4a581f3c.jpg""><br><img src=""paste-25d455699a81657d5eb2655f3ab3b66f1ab274d8.jpg""><br><img src=""paste-a7c480991774e92ab5d259ea5bad0c5722c45193.jpg""><br>Optimal Bayes solve this problem by applying this to space of all possible
hypothesis. We want to avoid this, so we don’t use total probability or heuristic
space of hypothesis. In order to do this, we assume that all the attributes are
independent each other given the dataset and classification value. This is a
very strong assumption:<br><img src=""paste-6b4e9a5e663eacce4969f85293e4c4ae1d713d61.jpg"">"
"We say that instances in a dataset (binary)
are <b>linearly separable</b> if"	there exists a linear function (hyperplane) such that the instance space is divided in two regions, in one you find only positive examples and in the other - only negative ones.
Can we use a combination of binary models for k-classes? Why?	"We can not use combination of binary linear models, in fact, given the figure, we have 3 classes but&nbsp;there is a green region in which you cannot
determinate the right classification&nbsp;(the “left one” says
that is C<sub>1</sub>, instead the “right one” says that that region is
C<sub>2</sub>). This is called One-versus-the-rest classifier in which
K-1 binary classifiers: C<sub>K</sub> vs not-C<sub>K</sub>.<br><img src=""paste-60c6ea7c2b40fecd68dbc57935e680e9edf22950.jpg"" style=""float: left;""><br><br><img src=""paste-a5f3ceec3737f5e157eca29a8d1f9d81868ae8c7.jpg"" width=""574"">"
How do we apply linear classification to multiple classes?	"We define a set of linear functions. Creating K-class disctiminant comprising of K linear functions (x not in dataset)&nbsp;<br><img src=""paste-3991315766575e4f0e12291e0dcf7fa20bd0498e.jpg""><br><img src=""paste-f718f206e7bfb44e71f939e58478ea7ad84eedaf.jpg""><img src=""paste-8325ea3dd4c499f27be5bbbdcf68f6c5759b38d7.jpg""><br>After, we learn linear discriminant"
Approaches to learn linear discriminants	"Given a multi-class classification problem and data set D with linearly
<br>separable data,<br><img src=""paste-ef600d1a2a3080c23748528d2cbacf4859eb75ff.jpg""><br><img src=""paste-3ad30a9b395896b4154c8df281acff2d23eda9c5.jpg""><br><img src=""paste-694d6fb982839b9f78f9bfb36dea6c80ca4b909f.jpg""><br><ul><li>Least squares</li><li>Perceptron</li><li>Fisher's linear discriminant</li><li>Support Vector Machines</li></ul>"
Explain least squares method	"<img src=""paste-9c3e2916accbf285317ddb271d7938e0dcc5b8ad.jpg""><img src=""paste-d676c35a354a55f84fd2cb917c04e6f18655f866.jpg""><br>pinv means pseudoinverse.<br>Our goal is to find th ematrix W.&nbsp;we need to solve this as an optimization
problem, so we will define an error function. This function is defined in terms of least square error, and the error is the difference between <b>prediction of the sample in the data and the value that is in the dataset<br></b><img src=""paste-ac00f8e54f9b305979342303e60f579e16d5f870.jpg""><br>"
Can we get a closed-form solution for the sum-of-squares error function for the Least Squares method? If yes, write it down	"<img src=""paste-292b7182f8b2ccc914999c47ece3af01fbe0f6a6.jpg""><br><img src=""paste-f58f87134d11cb7339c4eac291455f32aba4f009.jpg""><br>So the discriminant function will be a line dividing the classes. We can use it for classification.<br>"
In Least Squares method, how do we classify the new instance x not in the dataset	"<img src=""paste-0ea3f6b6874aa5bfb70ca57565f3dcdbfa833d1b.jpg"">"
Define and explain the the sum-of-squares error function in the context of the least squares method&nbsp;	"<img src=""paste-c255cf705623dd5577f32dd9873199dc3b38c417.jpg""><br><b>W</b> - weight matrix<br><b>X</b> - Input matrix. Rows - sample, Column - features<br><b>T</b> - Target matrix<br><b>XW-T</b> - matrix of residuals, difference between the predicted target values (obtained by multiplying the input matrix X by the weight matrix W) and the actual target values T.<br><b>(XW-T)<sup>T</sup></b>&nbsp;- transpose of the matrix of residuals<br><b>TR(.)</b> - trace operator, returns the sum of the diagonal elements of a square matrix.<br><b>1/2</b>&nbsp;- a scaling factor used t osimplify the computation of the gradient during optimization.&nbsp;<br><img src=""paste-28a25557efdc73cd5f40f89fac40e4a75923c30f.jpg"">"
Problem with the Least Squares method	"Minimizing error function E(W) is easy because we can easily solve it with a closed form solution. The problem is that this solution is not robust to outliers, because it finds the line that is&nbsp;<b>the best average distance from each points<br></b><img src=""paste-3404bdc5f6a4172bebbce3e0e9fbf21c1b63eeb5.jpg"">"
Explain Fishers linear discriminant in words	dimensionality reduction and classification technique used for linearly separable classes. It aims to find the linear combination of features that best separates multiple classes while preserving the class discrimination information as much as possible.&nbsp;
Explain Fisher's linear discriminant with precise definition	"<img src=""paste-698afd5f1163b71592583a730f087ae6bb106140.jpg"">"
Explain Perceptron model	"A perceptron, which is an iterative method, takes a vector of real-valued inputs, calculates a linear combination of these inputs, then outputs 1 if the result is greater than some threshold and -1 otherwise. We need to minimize the square
error (loss function).&nbsp;The output will be a function that
depends on the sign of the linear combination<br><img src=""paste-134dc2d92843fa79e4d75afbdfa2a625f1cc1efd.jpg"">"
State Perceptron training approaches. What are we trying to achieve?&nbsp;	Learning a perceptron involves choosing values for the weights w0, ... wn. Therefore, the space H of candidate hypotheses considered in perceptron learning is the set of all possible real-valued weight vectors.&nbsp;<br><br>For a single perceptron unit, the precise learning problem is to determine a weight vector that causes the perceptron to produce the correct +-1 output for each of the given training examples. Out of several algorithms, we consider 2:<br><ul><li>Perceptron Rule</li><li>Delta Rule</li></ul><br>
Explain Perceptron training rule	"<img src=""paste-9a40c2620e53dfbcb3b8b1bf84612ddfe7643475.jpg""><br><img src=""paste-e82e2c1b8d91fd0607d40817545d77e3c6103179.jpg""><br><img src=""paste-8e233d91b79bb9ce0ff584039707f7136055c9de.jpg""><br>We do derivative with respect to w_i and obtain:&nbsp;<br><img src=""paste-9638f4f92a236ae8e2b96d31a07e041900b85a4c.jpg""><br><img src=""paste-d048e7a75e2cd60c0accdb0a6d40d42e74c3bd4b.jpg"">"
How do we update weights for perceptron	"<img src=""paste-bd281ffe33fb38209431ea385de967693805748f.jpg"">"
Perceptron algorithm	"<img src=""paste-aa3904823d4e4fccc344c1442dd3e4ccf7106bed.jpg""><br><img src=""paste-f1a96202c6567dded6a1e7773e7d79275568a1f9.jpg""><br><img src=""paste-3fbe7b00eb5124235979bae2888607e00222f453.jpg"">"
When do we use Support Vector Machines?	when we dont want to find a line that completely divides instances simply, but which does it in the best way. We want a line that guarantees the maximum distance between points of the two classes from it.&nbsp;So, we want the hyperplane that maximize the margin between two points.
How do we compute SVM	"We have to make a scaling operation, so we scale all these points
by multiply all points by a constant term so this does not affect the solution.
And we rescale in such a way that the closest point has a distance from the
optimal hyperplane of 1. When we compute the optimal margin there will be 2
closest points, one for side, and these points will be at distance 1 from the
optimal hyperplane.&nbsp;"
State two probabilistic models for classification	"<ul><li><b>Generative -&nbsp;</b>&nbsp;estimate P(x | C<sub>i</sub>) and then compute P(C<sub>i&nbsp;</sub>| x) with Bayes<br></li><li><b>Discriminative -</b>&nbsp;estimates P(C<sub>i&nbsp;</sub>| x)</li></ul><div>The difference between generative and discriminative models lies in how they approach the estimation of the conditional probability <span style=""font-style: italic;"">P</span>(<span style=""font-style: italic;"">C</span><span style=""font-style: italic;"">i</span>​∣<span style=""font-style: italic;"">x</span>), where <span style=""font-style: italic;"">C</span><span style=""font-style: italic;"">i</span>​ represents a class label and <span style=""font-style: italic;"">x</span> represents the input data.<br></div>"
Explain Probabilistic Generative Models	"The idea is to compute the probability of a class given an instance by using Bayes theorem, and all the methods can be extended to multiple classes.<br><ol><li>&nbsp;Find the&nbsp;<b>conditional probability P(C<sub>i&nbsp;</sub>| x)&nbsp;</b><br><img src=""paste-443009c1b67d8b46a13e19e89512b17a014695c8.jpg""><br></li><li><b>Model the class-conditional distributions</b> - Now we assume that P(x | C<sub>i</sub>) can be replaced by a <b>gaussian distribution,&nbsp;</b>so we get that P(C<sub>i&nbsp;</sub>| x)&nbsp;is given by the sigmoid function of this term with means
and covariance of the two distributions.</li><li><b>Estimate Prior Probabilities P(C<sub>i</sub>)&nbsp;</b><br></li><li><b>Goal -&nbsp;</b>find the parameters of this model:&nbsp;so the means of the two gaussian distributions, sigma and covariance.
We just compute likelihood and then we solve the optimization problem for
finding max likelihood.<br></li></ol>"
<div><div><div><div><div><div><div><div>Explain Probabilistic Discriminative models</div></div></div></div></div></div></div></div>	"<div>They focus on directly modeling the conditional probability distribution of the class labels given the input features. Unlike generative models, which model the joint probability distribution of both the input features and the class labels, discriminative models aim to learn the decision boundary between different classes in the feature space.</div><div><ol><li><div><strong>Modeling Conditional Probability</strong>: Discriminative models directly estimate the conditional probability distribution <span style=""font-style: italic;"">P</span>(<span style=""font-style: italic;"">C</span><span style=""font-style: italic;"">i</span>​∣<span style=""font-style: italic;"">x</span>), which represents the probability of each class <span style=""font-style: italic;"">C</span><span style=""font-style: italic;"">i</span>​ given the input features <span style=""font-style: italic;"">x</span>. This conditional distribution captures the relationship between the input features and the class labels.</div></li><li>Discriminative models focus on learning the decision boundary that separates different classes in the feature space.</li></ol><div>We will estimate posterior probability directly without Bayes theorem. We
consider a dataset in a transformed space, the likelihood function is the
probability of receiving the output t given the input n, and the model is given by
the sigmoid function of a linear combination of the input and the weights.<br></div></div>"
What is a Logistic Regression?	"A probabilistic disctiminative model based on maximum likelihood.<br><img src=""paste-cada8f3b972ac669664e8ae422de6b73dc8e70db.jpg""><br><div>The cross-entropy error function, also known as the log loss or logistic loss, is commonly used in binary classification problems, particularly in logistic regression and neural networks with binary output units. It measures the difference between the predicted probability distribution and the actual distribution of the target variable.</div><div>In the binary classification setting, where the target variable �<span style=""font-style: italic;"">y</span> takes on values of 0 or 1, the cross-entropy error function is defined as:</div><div><img src=""paste-80106c0b9d3d1865d09e4f2f89ebb7637e6bd2fa.jpg""><br></div><div>In multi-class classification problems, the cross-entropy error function is extended to handle multiple classes using the softmax function and is calculated over all classes.<br></div>"
What is linear regression?&nbsp;	Linear regression is a statistical method used to model the relationship between a dependent variable and one or more independent variables by fitting a linear equation to observed data.
What is the formula for a linear regression model?&nbsp;	"<div>The formula for a simple linear regression model with one independent variable is: <span style=""font-style: italic;"">y</span>=<span style=""font-style: italic;"">m</span><span style=""font-style: italic;"">x</span>+<span style=""font-style: italic;"">b</span>, where <span style=""font-style: italic;"">y</span> is the dependent variable, <span style=""font-style: italic;"">x</span> is the independent variable, <span style=""font-style: italic;"">m</span> is the slope of the line, and <span style=""font-style: italic;"">b</span> is the y-intercept.</div><br>"
What is the least squares method?	<div>The least squares method is a technique used to estimate the parameters of a linear regression model by minimizing the sum of the squared differences between the observed and predicted values of the dependent variable.</div><br>
What is the normal equation in linear regression?&nbsp;	"<div>The normal equation is a closed-form solution for finding the optimal parameters of a linear regression model by solving a system of linear equations. It is expressed as: <span style=""font-style: italic;"">θ</span>=(<span style=""font-style: italic;"">X</span><sup><span style=""font-style: italic;"">T</span></sup><span style=""font-style: italic;"">X</span>)<sup>−1</sup><span style=""font-style: italic;"">X</span><sup><span style=""font-style: italic;"">T</span></sup><span style=""font-style: italic;"">y</span>, where <span style=""font-style: italic;"">θ</span> is the vector of parameters, <span style=""font-style: italic;"">X</span> is the matrix of independent variables, and <span style=""font-style: italic;"">y</span> is the vector of dependent variable values.</div><br>"
What is the difference between linear regression and logistic regression?&nbsp;	<div>Linear regression is used for predicting continuous values, while logistic regression is used for predicting binary outcomes. Linear regression uses a linear equation to model the relationship between variables, while logistic regression uses the logistic function to model the probability of a binary outcome.</div>
What is the loss function used in linear regression?&nbsp;	<div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div>The cost function used in linear regression is the mean squared error (MSE) function, which calculates the average of the squared differences between the predicted and actual values of the dependent variable. It is minimized to find the optimal parameters of the model.</div></div></div></div></div></div></div></div></div><br>The least squares method uses the Mean Squared Error (MSE) as its loss function. The goal of the least squares method in linear regression is to minimize the MSE, which represents the average squared difference between the actual target values and the predicted values generated by the linear regression model. By minimizing the MSE, the least squares method aims to find the optimal parameters (coefficients) for the linear regression model that best fit the training data.</div></div></div></div><div><div><div></div><div></div></div></div><div></div></div><div><div></div></div></div></div></div></div></div></div></div></div><div><div><div></div><div><br></div></div></div>
<div>What are basis functions in linear regression?</div><br>	<div><div><div><div><div><div><div><div><div>Basis functions in linear regression are functions used to transform the input features into a higher-dimensional space before fitting the linear regression model. They allow the model to capture nonlinear relationships between the input variables and the target variable. Basis functions can include polynomial functions, radial basis functions, Fourier basis functions, etc. They help increase the flexibility of the linear regression model to better fit complex data patterns.</div></div></div></div><div><div><div></div></div></div><div></div></div><div><div></div></div></div></div></div></div><div><div><div><div><div><div><div><br></div></div></div></div></div></div></div>
<div>Are basis functions linear or nonlinear?</div>	<div><div><div><div><div><div><div><div><div><div><div><div><div>Basis functions can be either linear or nonlinear. In linear regression, the combination of basis functions with linear coefficients results in a linear relationship between the input features and the target variable. However, the basis functions themselves can be nonlinear transformations of the input features.</div></div></div></div><div><div><div></div><div></div></div></div><div></div></div><div><div></div></div></div></div></div></div></div></div></div></div><div><div><div></div><div><br></div></div></div>
<div>Explain the least squares error minimization</div>	<div><div><div><div><div><div><div><div><div><div><div><div><div>In the context of linear regression, the least squares error minimization aims to find the optimal parameters (coefficients) of the model that minimize the sum of the squared differences between the predicted values and the actual target values in the training dataset. Mathematically, this involves minimizing the sum of the squared residuals, often expressed as the <b>mean squared error (MSE).</b> This process is typically achieved using optimization techniques such as <b>gradient descent</b> or <b>closed-form solutions like the normal equation</b>. The goal is to find the coefficients that best fit the training data and generalize well to unseen data.</div></div></div></div><div><div><div></div><div></div></div></div><div></div></div><div><div></div></div></div></div></div></div></div></div></div></div><div><div><div></div><div><br></div></div></div>
What is a Markov Decision Process (MDP)?	<div>An MDP is a mathematical framework used to model decision-making problems in which an agent interacts with an environment over time. It consists of a set of states, actions, transition probabilities, and rewards, and satisfies the Markov property, meaning that the future state depends only on the current state and action.</div>
<div>What are the components of a Markov Decision Process (MDP)?</div>	<div>The components of an MDP include:<br><ol><li>States (S): The set of possible situations or configurations the agent can be in.</li><li>Actions (A): The set of possible decisions or choices the agent can make.</li><li>Transition Probabilities (P): The probabilities of transitioning from one state to another after taking a specific action.</li><li>Rewards (R): The immediate numerical feedback the agent receives after taking an action in a particular state.</li><li>Policy (π): The strategy or rule the agent uses to select actions in each state.</li></ol></div>
<div>What is the goal of Reinforcement Learning (RL)?</div>	<div>The goal of RL is to learn a policy that maximizes the cumulative reward obtained by an agent over time in an uncertain and potentially complex environment. The agent interacts with the environment by taking actions and receiving feedback in the form of rewards, with the objective of learning to make decisions that lead to desirable outcomes.</div>
<div>What are the key components of Reinforcement Learning (RL)?</div>	<div>The key components of RL include:</div><ol><li>Agent: The entity that learns to make decisions by interacting with the environment.</li><li>Environment: The external system with which the agent interacts.</li><li>State (s): A representation of the current situation or configuration of the environment.</li><li>Action (a): The decision made by the agent at each time step.</li><li>Reward (r): The immediate feedback provided by the environment after each action.</li><li>Policy (π): The strategy or rule used by the agent to select actions in different states.</li></ol>
the difference between Supervised Learning and Reinorcement Learning&nbsp;	"... is in the dataset.&nbsp;<br><img src=""paste-72eab9e1d63795bf811b4e4eb6a5d78e234fe8f1.jpg"">"
<div>What is the Markov property?</div>	The Markov property states that the future state of a system depends only on the current state and not on the sequence of events that preceded it. In other words, given the present state, the future state is conditionally independent of the past states.
<div>What is a deterministic Markov Decision Process (MDP)?</div><br>	<div>In a deterministic MDP, the outcome of each action is completely determined by the current state and the action taken, without any uncertainty. This means that for each state-action pair, there is a single next state and reward associated with it.</div>
<div>What is a non-deterministic Markov Decision Process (MDP)?</div>	<div>In a non-deterministic MDP, the outcome of an action in a given state is not completely predictable. Instead, there is a probability distribution over possible next states and rewards for each state-action pair. This introduces uncertainty into the decision-making process.</div><br>
<div>What is an Optimal Policy?</div><br>	<div><div><div><div><div><div><div><div><div><div><div><div><div>An optimal policy in reinforcement learning is a policy that maximizes the expected cumulative reward in a Markov Decision Process (MDP). It specifies the best action to take in each state to achieve the highest possible reward over time.</div></div></div></div><div><div><div></div><div></div></div></div><div></div></div><div><div></div></div></div></div></div></div></div></div></div></div><div><div><div></div><div><br></div></div></div>
<div>Optimization Function in MDP</div><br>	<div>An optimization function in a Markov Decision Process (MDP) is a function used to maximize or minimize an objective, such as the expected cumulative reward. It guides the decision-making process by determining the best actions to take in each state to achieve the desired outcome.</div><br>
<div>Flashcard: Discount Factor</div><br>	<div>The discount factor (usually denoted as γ) is a parameter used in reinforcement learning to discount future rewards. It represents the preference of immediate rewards over future rewards and affects the agent's decision-making process by determining how much weight is given to future rewards in the agent's value estimation. A discount factor of 0 means the agent only cares about immediate rewards, while a discount factor of 1 means the agent values all future rewards equally. Typical values for the discount factor range between 0 and 1.</div><br>
<div>Learning in MDP</div><br>	"<div>Learning in a Markov Decision Process (MDP) refers to the process by which an agent improves its policy or action-selection strategy over time through interaction with the environment. The goal of learning in MDPs is typically to find an optimal policy that maximizes the expected cumulative reward.</div><div>Example:
Q-Learning is a popular learning algorithm used in reinforcement learning to learn the optimal action-value function in an MDP without requiring a model of the environment.</div><br>"
Policy Iteration	<div>Policy iteration is an iterative algorithm used to find the optimal policy in a Markov Decision Process (MDP). It involves alternating between policy evaluation and policy improvement steps until convergence.</div><br>
Q-Function in MDP	"The Q-function, also known as the action-value function, is a function in reinforcement learning that measures the expected cumulative reward of taking an action <span style=""font-style: italic;"">a</span> in a state <span style=""font-style: italic;"">s</span> and following a specific policy thereafter. It is denoted as <span style=""font-style: italic;"">Q</span>(<span style=""font-style: italic;"">s</span>,<span style=""font-style: italic;"">a</span>).<br><br><div>Explanation:</div><ul><li>The Q-function represents the expected future rewards of selecting action&nbsp;<span style=""font-style: italic;"">a</span> in state&nbsp;<span style=""font-style: italic;"">s</span> and following a particular policy thereafter.</li><li>It plays a crucial role in many reinforcement learning algorithms, including Q-learning and policy iteration.</li><li>The goal of reinforcement learning is often to find the optimal Q-function, which provides the maximum expected cumulative reward for each state-action pair.</li></ul><div>Example:
In a grid-world environment, the Q-function&nbsp;<span style=""font-style: italic;"">Q</span>(<span style=""font-style: italic;"">s</span>,<span style=""font-style: italic;"">a</span>) would represent the expected cumulative reward of moving from state&nbsp;<span style=""font-style: italic;"">s</span> to the next state by taking action&nbsp;<span style=""font-style: italic;"">a</span>, considering the rewards and transition probabilities associated with each action.</div>"
<div>Stochastic Model in MDP</div><br>	A stochastic model in Markov Decision Processes (MDPs) is a model where transitions between states are subject to randomness or uncertainty. In a stochastic model, the outcome of taking an action in a particular state is probabilistic, with transition probabilities defining the likelihood of moving to each possible next state. Stochastic models are commonly used in environments where there is inherent uncertainty or variability in the system dynamics, making them essential for modeling real-world scenarios in reinforcement learning.
<div><div>Q-learning Algorithm for Deterministic MDPs</div><br></div>	"<div>Q-learning is a reinforcement learning algorithm used to find an optimal action-selection policy for a given MDP. In deterministic MDPs, where actions lead to deterministic outcomes, Q-learning iteratively updates the Q-values (state-action values) using the Bellman optimality equation:</div><div><img src=""paste-e78424208a1c9f6ea4dea48914daa1060b1fde29.jpg""><br></div><div>Here,&nbsp;<span style=""font-style: italic;"">Q</span>(<span style=""font-style: italic;"">s</span>,<span style=""font-style: italic;"">a</span>) represents the Q-value for taking action <span style=""font-style: italic;"">a</span> in state <span style=""font-style: italic;"">s</span>, <span style=""font-style: italic;"">r</span> is the immediate reward, <span style=""font-style: italic;"">γ</span> is the discount factor, <span style=""font-style: italic;"">s</span>′ is the next state, and �′<span style=""font-style: italic;"">a</span>′ is the next action. <span style=""font-style: italic;"">α</span> is the learning rate, controlling the weight of new information in the Q-value updates.<br></div><div><br></div>"
<div>Exploration vs Exploitation</div><br>	<div>Exploration and exploitation are two key concepts in reinforcement learning (RL) and multi-armed bandit problems:</div><ol><li><div>Exploration: Refers to the strategy of trying out different actions to gather more information about the environment or to discover potentially better actions. It involves taking actions that may not seem optimal based on current knowledge.</div></li><li><div>Exploitation: Involves taking actions that are believed to be the best based on current knowledge to maximize immediate rewards or outcomes. It focuses on exploiting the known information to achieve short-term gains.</div></li></ol><div>Balancing exploration and exploitation is crucial in RL tasks. Too much exploration may result in inefficient learning, while too much exploitation may lead to suboptimal solutions.</div>
<div>ε-Greedy Strategy</div><br>	<div>The ε-greedy strategy is a common exploration-exploitation strategy used in reinforcement learning and multi-armed bandit problems. It balances exploration and exploitation by choosing between two actions:</div><ol><li><div>Exploitation: With probability 1-ε, the agent selects the action that is currently estimated to be the best based on its knowledge (exploiting).</div></li><li><div>Exploration: With probability ε, the agent selects a random action from the action space, allowing it to explore other options (exploring).</div></li></ol><div>The value of ε determines the trade-off between exploration and exploitation. A higher ε encourages more exploration, while a lower ε favors exploitation.</div>
<div>Softmax Strategy</div><br>	<div>The softmax strategy, also known as the Boltzmann exploration, is an exploration-exploitation strategy used in reinforcement learning. It selects actions probabilistically based on their estimated values.</div><div>Given a set of action values Q(a), the softmax action selection computes the probability of selecting each action using the softmax function:</div><div>P(a) = exp(Q(a) / τ) / Σ[exp(Q(a') / τ)]</div><div>where:</div><ul><li>P(a) is the probability of selecting action a,</li><li>exp is the exponential function,</li><li>Q(a) is the estimated value of action a,</li><li>τ is the temperature parameter that controls the level of exploration (higher τ encourages more exploration),</li><li>Σ denotes summation over all actions a' in the action space.</li></ul><div>The softmax strategy ensures that actions with higher estimated values have higher probabilities of being selected, while still allowing for exploration.</div><br>
<div>Non-Deterministic Reinforcement Learning</div>	Non-deterministic reinforcement learning involves situations where the outcomes of actions are uncertain due to probabilistic transitions between states. Key concepts include:<br><br><ul><li>Rewards depending on current and successor states.</li><li>Transition and rewards functions being nondeterministic.</li><li>The value function is defined as the expected value of the cumulative discounted reward.</li><li>The optimal policy is derived from maximizing expected future rewards.</li><li>Training rules update the Q-function, which estimates the expected cumulative reward.</li><li>Algorithms like Temporal Difference and SARSA handle non-deterministic learning.</li><li>Policy iteration and Policy Gradient Algorithm are alternative approaches to value iteration for non-deterministic environments.</li></ul>
<div>Value Function in Non-Deterministic RL</div>	"<div>In non-deterministic reinforcement learning, the value function represents the expected cumulative discounted reward. It accounts for the uncertainty in transitions between states and the stochastic nature of rewards.</div><img src=""paste-f718bfb093c634f63a5bf1a321f7c0dae6a106fd.jpg""><br><img src=""paste-183f9595704a2068a4384dab59563698595ac473.jpg""><br>When there are multiple states, you need to consider also the best I can do in
the future. The difference from the deterministic one is that this term contains
the immediate rewards plus gamma time the best I can do in the future. (*) this
is the estimation of what I can get, so is the actual reward plus gamma times
what I could get in the future, (**) instead is what I believed before executing
an action on x. So, the difference is the error between my estimation and what
I observe after the execution of the action.<br>"
SARSA	"a variant of Q-learning in which
instead of trying to compute the best I can do with optimal policy I just compute
the best I can do with current policy, so instead of considering max of all
possible actions on Q-table I just choose an action without using the max
operator"
"There are other
algorithms for non-deterministic learning like <span class=""cloze"" data-cloze=""Temporal&#x20;Difference"" data-ordinal=""1"">[...]</span>, where
instead of only looking of what happens in next state I want to try to look in
more than one state, so, instead of computing one step time difference I can
compute n-step time difference, or <span class=""cloze"" data-cloze=""SARSA"" data-ordinal=""1"">[...]</span> a variant of Q-learning in which
instead of trying to compute the best I can do with optimal policy I just compute
the best I can do with current policy, so instead of considering max of all
possible actions on Q-table I just choose an action without using the max
operator."	"There are other
algorithms for non-deterministic learning like <span class=""cloze"" data-ordinal=""1"">Temporal Difference</span>, where
instead of only looking of what happens in next state I want to try to look in
more than one state, so, instead of computing one step time difference I can
compute n-step time difference, or <span class=""cloze"" data-ordinal=""1"">SARSA</span> a variant of Q-learning in which
instead of trying to compute the best I can do with optimal policy I just compute
the best I can do with current policy, so instead of considering max of all
possible actions on Q-table I just choose an action without using the max
operator.<br>
"
"We can even do Reinforcement learning with <span class=""cloze"" data-cloze=""Policy&#x20;iteration"" data-ordinal=""1"">[...]</span>, in which we use
directly π instead of V(x) or Q(x, a), so with these methods we can overcome
some difficulties in value iteration but they are more complicated to execute."	"We can even do Reinforcement learning with <span class=""cloze"" data-ordinal=""1"">Policy iteration</span>, in which we use
directly π instead of V(x) or Q(x, a), so with these methods we can overcome
some difficulties in value iteration but they are more complicated to execute.<br>
"
"In the <span class=""cloze"" data-cloze=""Policy&#x20;Gradient&#x20;Algorithm"" data-ordinal=""1"">[...]</span> we compute the gradient of the policy in a
local interval with respect to current point and then move in order to improve
value of this policy. So, starting from a random policy we change parameters
in order to move to a better different one"	"In the <span class=""cloze"" data-ordinal=""1"">Policy Gradient Algorithm</span> we compute the gradient of the policy in a
local interval with respect to current point and then move in order to improve
value of this policy. So, starting from a random policy we change parameters
in order to move to a better different one<br>
"
"The <span class=""cloze"" data-cloze=""Hidden&#x20;Markov&#x20;model"" data-ordinal=""1"">[...]</span> is a model useful to describe a system in which
state are not observable. We have still a Markov chain, that it’s just an
assumption that current state contains all information needed to understand,
but now we don’t make any assumptions about the fully observability of the
states, in fact agent cannot look into the states and cannot understand exactly
the current configuration. For each state we have a dependency with the
observation. In fact states x<sub>t</sub> are discrete and <span class=""cloze"" data-cloze=""non&#x2D;observable"" data-ordinal=""1"">[...]</span> and the
observations z<sub>t</sub> can be <span class=""cloze"" data-cloze=""discrete"" data-ordinal=""1"">[...]</span> or <span class=""cloze"" data-cloze=""continuous"" data-ordinal=""1"">[...]</span>, and controls ut are not present
so evolution is not controlled by our system. So the interest in this model is to
understand the state in which we are."	"The <span class=""cloze"" data-ordinal=""1"">Hidden Markov model</span> is a model useful to describe a system in which
state are not observable. We have still a Markov chain, that it’s just an
assumption that current state contains all information needed to understand,
but now we don’t make any assumptions about the fully observability of the
states, in fact agent cannot look into the states and cannot understand exactly
the current configuration. For each state we have a dependency with the
observation. In fact states x<sub>t</sub> are discrete and <span class=""cloze"" data-ordinal=""1"">non-observable</span> and the
observations z<sub>t</sub> can be <span class=""cloze"" data-ordinal=""1"">discrete</span> or <span class=""cloze"" data-ordinal=""1"">continuous</span>, and controls ut are not present
so evolution is not controlled by our system. So the interest in this model is to
understand the state in which we are.<br>
"
"We want to solve the state estimation problem, we can represent this HMM
like this:"	"<img src=""paste-9e52b4389b4712cf4eb2d747cea3b0ffb1608712.jpg""><br>We have set of states X, a set of observations Z, and an initial probability
distribution for the initial state. Then we have a transition model that is a
probability distribution that denote dynamic evolution of the system (like MDP&nbsp;but without actions). The observation model is the probability of an
observation given a state."
<div><strong>Hidden Markov Model (HMM)</strong></div>	A probabilistic model used to describe systems where states are not directly observable but are related to observable outputs through probabilities.
<strong>HMM Components</strong>	<ul><li>States: Discrete and non-observable entities in the model.</li><li>Observations: Observable outputs, which may be discrete or continuous.</li><li>Initial Distribution: Probability distribution for the initial state.</li><li>Transition Model: Probability distribution describing the dynamic evolution of the system.</li><li>Observation Model: Probability of observing an output given a state.</li></ul>
<strong>HMM Parameters Estimation</strong>	<ul><li>If states are observed at training time:<ul><li>Transition Matrix: Estimates the probability of transitions between states.</li><li>Observation Matrix: Estimates the probability of observing outputs given states.</li></ul></li><li>If states are not observed at training time:<ul><li>Baum-Welch Algorithm: An EM-based method for estimating parameters when states are not observable.</li></ul></li></ul>
<strong>State Estimation Problems</strong>	<ul><li>Filtering: Estimating the current state given all observations so far.</li><li>Smoothing: Estimating past states given all observations.</li></ul><br>
<div><strong>POMDP (Partially Observable Markov Decision Process):</strong></div><ul><br></ul>	A model combining elements of MDP and HMM, where states are partially observable but actions can be taken to influence the system.
<strong>Belief State</strong>	A probability distribution over possible states, representing the agent's uncertainty about the current state.
<strong>POMDP Solution Methods</strong>	"One way to solve POMDP is to model states in terms of probability
distributions, by defining the state of belief states and build an MDP that has
the set of belief states, sort this MDP and compute the policy. The problem is
that set of belief states is continuous.<br><strong><br></strong>We know that belief states are infinite, so we need to discretize them, and
there exist several methods to do this<strong><br><br>Value Iteration</strong>: Value iteration is a dynamic programming method used to solve MDPs by iteratively updating the value function until it converges to the optimal values.<br><br><strong>Policy Iteration</strong>: Policy iteration is another dynamic programming method that alternates between policy evaluation and policy improvement steps until convergence.&nbsp;<br><strong><br>Q-Learning and Reinforcement Learning</strong>: Q-learning and reinforcement learning techniques can also be adapted for POMDPs by using function approximation methods to estimate the value function or policy. This includes methods like Deep Q-Networks (DQN) and Deep Reinforcement Learning from Demonstrations (DRLfD)."
<div><strong>Value Function and Optimal Policy:</strong></div>	Definition: Value function represents the expected cumulative reward from a given belief state, guiding decision-making in POMDPs.
<strong>Kernel Methods</strong>	Description: Kernel methods are a set of powerful techniques in machine learning used to handle complex relationships between features. They allow linear models to capture non-linear patterns in data by introducing a notion of similarity between instances through kernel functions.
<strong>Kernel Function</strong>	Description: A kernel function is a mathematical function that measures the similarity between pairs of instances in a dataset. It quantifies how alike two instances are in feature space, enabling kernel methods to operate on high-dimensional data without explicitly mapping to a higher-dimensional space.
<strong>Kernelized Linear Model:</strong>	Description: A kernelized linear model extends traditional linear models by incorporating kernel functions to compute inner products between instances. This approach enables the model to capture intricate relationships between features, offering greater flexibility in modeling complex datasets.
<strong>Kernel Trick</strong>	Description: The kernel trick is a clever method used in machine learning to apply linear algorithms to non-linear problems. By substituting inner products with kernel functions, the kernel trick allows linear models to implicitly operate in a higher-dimensional feature space, effectively capturing non-linear patterns in data.
<strong>Gram Matrix</strong>	A Gram matrix is like a summary table for a dataset, where each entry represents the similarity between pairs of instances. Specifically, it's a symmetric matrix where each element corresponds to the inner product (or similarity) between two instances in the dataset. In kernelized linear models, the Gram matrix is fundamental because it encapsulates all the pairwise similarities between instances, allowing the model to efficiently compute solutions by leveraging these inner products in feature space. In essence, the Gram matrix provides a concise representation of the dataset's relationships, which is crucial for effectively applying kernel methods to linear models.
<strong>RBF Kernel</strong>	<div><div><div><div><div><div><div><div><div><div><div><div><div>The Radial Basis Function (RBF) kernel is a popular choice among kernel functions in kernel methods. It's particularly effective because it transforms input data into a higher-dimensional space in a way that highlights complex patterns and relationships. This transformation is essential for handling datasets with intricate, non-linear structures that cannot be effectively separated by simple linear models. By applying the RBF kernel, the model gains the ability to capture these nuanced relationships and make more accurate predictions, even in situations where linear methods fall short.</div></div></div></div></div></div></div></div></div></div></div></div></div>
<strong>RBF Kernel</strong>	The Radial Basis Function (RBF) kernel is a popular choice among kernel functions in kernel methods. It's particularly effective because it transforms input data into a higher-dimensional space in a way that highlights complex patterns and relationships. This transformation is essential for handling datasets with intricate, non-linear structures that cannot be effectively separated by simple linear models. By applying the RBF kernel, the model gains the ability to capture these nuanced relationships and make more accurate predictions, even in situations where linear methods fall short
What are non-parametric models and how do they differ from parametric models?&nbsp;	Non-parametric models are models where the number of parameters grows with the amount of data, unlike parametric models where the number of parameters is fixed. In non-parametric models, the model complexity adapts to the complexity of the data, making them more flexible.
Can you give an example of a non-parametric model?&nbsp;	<div>Instance-based learning, such as K-nearest neighbors (K-NN), is an example of a non-parametric model. In K-NN, the number of parameters grows with the size of the dataset, allowing the model to adapt to the data without assuming a fixed model structure.</div>
How does K-nearest neighbors (K-NN) classification work?&nbsp;	<div>K-nearest neighbors (K-NN) classifies a new instance by finding the majority class among its k nearest neighbors in the dataset. The prediction is based on the most common class label among the neighbors.</div>
How can the nearest neighbor problem be kernelized, and what does it involve?&nbsp;	<div><div><div><div><div><div><div><div><div><div><div><div><div><div>Kernelizing the nearest neighbor problem involves replacing the traditional distance function with a kernel function. This kernel function computes the similarity between instances in a high-dimensional feature space. Unlike a distance function, which directly measures the distance between instances, a kernel function quantifies the similarity between instances based on their features. By using a kernel function, we can capture complex relationships and nonlinear patterns in the data, allowing us to make more accurate predictions in the nearest neighbor algorithm. This approach enables us to incorporate domain knowledge and tailor the similarity measure to the specific characteristics of the dataset, improving the performance of the nearest neighbor method.</div></div></div></div></div></div></div></div></div></div></div></div></div></div>
What is the difference between K-NN for classification and regression?&nbsp;	In K-nearest neighbors (K-NN) classification, the majority class among the k nearest neighbors is chosen as the prediction, while in regression, the average (or another statistical measure) of the target values of the k nearest neighbors is used for prediction. K-NN regression involves fitting a constant model to the target values of the nearest neighbors.
What is the significance of the Gram matrix in kernelized linear models?&nbsp;	<div>The Gram matrix plays a crucial role in kernelized linear models by efficiently representing the inner products between instances in the feature space. It enables the computation of solutions by directly operating on the pairwise inner products of instances.</div><br>
Explain the role of the Radial Basis Function (RBF) kernel in kernel methods.&nbsp;	<div>The Radial Basis Function (RBF) kernel is a kernel function commonly used in kernel methods to transform input data into a higher-dimensional space. By mapping data into a higher-dimensional space, the RBF kernel makes it easier to capture complex patterns and non-linear relationships in the data.</div><br>
Can you provide an example of how the RBF kernel is used in practice?&nbsp;	<div>One example of using the RBF kernel is in support vector machines (SVMs) for classification tasks. The RBF kernel allows SVMs to model complex decision boundaries by implicitly mapping input data into a higher-dimensional feature space where linear separation is possible.</div><br>
How does instance-based learning, such as K-nearest neighbors (K-NN), handle prediction?&nbsp;	<div>In instance-based learning, such as K-nearest neighbors (K-NN), prediction is based on the similarity between new instances and instances in the training dataset. The model identifies the k nearest neighbors to the new instance and makes predictions based on the majority class (for classification) or the average target value (for regression) of these neighbors.</div><br>
What is the key difference between parametric and non-parametric models?&nbsp;	<div><div><div><div><div><div><div><div><div>The key difference between parametric and non-parametric models is in the number of parameters. Parametric models have a fixed number of parameters independent of the size of the dataset, while non-parametric models have a number of parameters that grow with the amount of data, allowing them to adapt to complex datasets.</div></div></div></div><div><div><div></div></div></div><div></div></div><div><div></div></div></div></div></div></div><div><div><div><div><div><div><div><br></div></div></div></div></div></div></div>
<div><strong>Artificial Neural Networks (ANN)</strong>:</div>	<ul><li>Definition: ANN represent a family of parametric models used for classification and regression tasks, where an error function is defined and optimized to find the minimum error.</li><li>Features: Flexible architecture allowing for non-linear function approximation.</li></ul>
<div><strong>Feedforward Neural Network (FNN)</strong>:</div>	<ul><li>Definition: FNN is a type of neural network with a layered architecture, where connections only flow from input to output layers, and no loops exist.</li><li>Constraints: Each unit is connected only to the next layer, and no connections exist between non-adjacent layers.</li></ul>
<div><strong>Universal Approximation Theorem</strong>:</div>	<ul><li>Theorem: States that a neural network with a single hidden layer, given a sufficient number of hidden units, can approximate any function with arbitrary precision.</li><li>Implication: Highlights the expressive power of neural networks in function approximation tasks.</li></ul>
<div><strong>Activation Functions</strong>:</div>	<ul><li>RELU: Rectified Linear Unit, commonly used in hidden layers due to computational efficiency and non-linearity.</li><li>Sigmoid: Logistic function used in binary classification output layers.</li><li>Softmax: Used in multi-class classification output layers to generate probability distribution over classes.</li></ul>
<div><strong>Back-Propagation Algorithm</strong>:</div>	<ul><li>Definition: An algorithm used to compute gradients of the loss function with respect to the network parameters.</li><li>Phases: Forward phase computes predictions and loss, while backward phase propagates gradients to update parameters.</li><li>Importance: Crucial for training neural networks by iteratively updating parameters to minimize error.</li></ul>
<div><strong>Regularization Techniques</strong>:</div>	<ul><li>Definition: Methods to prevent overfitting by imposing constraints on the model parameters or dataset.</li><li>Examples: Dropout, Early Stopping, Parameter Sharing, Data Augmentation.</li></ul>
<div><strong>Stochastic Gradient Descent (SGD)</strong>:</div>	<ul><li>Definition: Optimization algorithm that updates parameters based on gradients computed from mini-batches of the dataset.</li><li>Advantage: Faster convergence and reduced memory requirements compared to batch gradient descent.</li></ul>
<strong>Overfitting Prevention</strong>	<ul><li>Definition: Overfitting occurs when a model learns the training data too well, capturing noise and producing poor generalization.</li><li>Techniques: Regularization, Data Augmentation, Dropout, Early Stopping.</li></ul>
<div><strong>Momentum in Optimization</strong>:</div><ul><br></ul>	<ul><li>Definition: Momentum is a technique used in optimization algorithms to accelerate convergence by adding a fraction of the previous update to the current update.</li><li>Purpose: Helps overcome local minima and saddle points by allowing the optimization process to continue moving in the direction of the gradient.</li></ul>
<div><strong>Local Minima in Optimization</strong>:</div><ul><br></ul>	<ul><li>Definition: Local minima are points in the parameter space where the error function has a lower value than in the immediate vicinity.</li><li>Challenge: Optimization algorithms like gradient descent may get stuck in local minima, preventing convergence to the global minimum.</li><li>Solutions: Momentum, Learning Rate Scheduling, Random Restart, Simulated Annealing.</li></ul>
<ul><div><strong>Mini-Batch Gradient Descent</strong>:</div><ul><br></ul></ul>	<ul><li>Definition: Variant of gradient descent where updates are made based on gradients computed from mini-batches of the dataset.</li><li>Benefits: More stable convergence compared to stochastic gradient descent, faster training compared to batch gradient descent.</li><li>Parameters: Mini-batch size determines the number of samples used to compute each gradient update.</li></ul>
<div><strong>Learning Rate Scheduling</strong>:</div><ul><br></ul>	<ul><li>Definition: Technique used to adjust the learning rate during training to improve convergence and stability.</li><li>Methods: Step Decay, Exponential Decay, Linear Decay, Cyclical Learning Rates.</li><li>Importance: Helps balance between fast convergence and avoiding overshooting the minima.</li></ul>
<div><strong>Data Augmentation</strong>:</div><ul><br></ul>	<ul><li>Definition: Process of artificially expanding the training dataset by applying transformations such as rotation, scaling, and flipping to existing data samples.</li><li>Purpose: Increases the diversity of the training data, improving model generalization and robustness. Also can help mitigate overfitting</li><li>Applications: Commonly used in computer vision tasks to enhance model performance.&nbsp;</li></ul>
<div><strong>Early Stopping</strong>:</div><ul><br></ul>	<ul><li>Definition: Technique used to prevent overfitting by monitoring the model's performance on a validation set during training and stopping the training process when performance starts to degrade.</li><li>Benefits: Helps prevent the model from memorizing noise in the training data, improves generalization to unseen data.</li><li>Implementation: Monitor the validation loss or other performance metrics and stop training when no improvement is observed for a certain number of epochs.</li></ul>
<div><strong>Parameter Sharing</strong>:</div><ul><br></ul>	<ul><li>Definition: Strategy used to reduce the number of learnable parameters in a model by constraining certain parameters to be shared across different parts of the network.</li><li>Purpose: Reduces model complexity and prevents overfitting, especially in convolutional neural networks (CNNs).</li><li>Example: Sharing weights across convolutional filters in CNNs to enforce translation invariance.</li></ul>
<div><strong>Dropout Regularization</strong>:</div><ul><br></ul>	<ul><li>Definition: Technique used to prevent overfitting in neural networks by randomly dropping a fraction of neurons (along with their connections) during training.</li><li>Purpose: Forces the network to learn more robust features and reduces co-dependency among neurons.</li><li>Implementation: Applied during training phase only, with dropout probability typically set between 0.2 and 0.5.</li></ul>
<ul><div><strong>Activation Functions</strong>:</div><ul><br></ul></ul>	<ul><li>Definition: Mathematical functions applied to the output of neurons in a neural network to introduce non-linearity and enable the network to learn complex relationships.</li><li>Common Types: ReLU (Rectified Linear Unit), Sigmoid, Tanh (Hyperbolic Tangent), Softmax.</li><li>Selection Criteria: Determined based on the nature of the problem, network architecture, and computational considerations.</li></ul>
<div><div><strong>Convolutional Neural Networks (CNN)</strong>:</div></div>	<ul><li>Definition: A type of neural network designed for processing structured grid data, such as images or audio signals, by using convolutional layers.</li><li>Components: Convolutional stage, Detector stage, Pooling stage.</li><li>Purpose: Effective in capturing spatial hierarchies of features and handling translation-invariant inputs.</li></ul>
<div><strong>Convolutional Stage</strong>:</div><ul><br></ul>	<ul><li>Definition: The initial stage in a CNN where convolutional filters are applied to input data to extract features.</li><li>Techniques: Utilizes sparse connectivity and parameter sharing to improve performance and reduce overfitting.</li><li>Operation: Applies convolutional filters over input data to produce feature maps.</li></ul>
<div>Detector Stage:</div><br>	<ul><li>Definition: The stage in a CNN following the convolutional stage where non-linear transformations are applied to the output of convolutional layers.</li><li>Activation Functions: Commonly used functions include ReLU (Rectified Linear Unit) or tanh (Hyperbolic Tangent).</li><li>Purpose: Introduces non-linearity and enhances the network's ability to capture complex patterns.</li></ul>
<div>Pooling Stage:</div><br>	<ul><li>Definition: The stage in a CNN where pooling operations are applied to feature maps to downsample the data and introduce invariance to small transformations.</li><li>Operation: Typically performs operations like max pooling or average pooling over local regions of the input.</li><li>Purpose: Reduces the spatial dimensions of the data while preserving important features.</li></ul>
<div><div>Pooling Operation:</div><br></div>	<ul><li>Definition: A fixed operation applied to feature maps in the pooling stage of a CNN to downsample the data and introduce spatial invariance.</li><li>Types: Includes operations like max pooling and average pooling, where the maximum or average value within local regions is retained.</li><li>Purpose: Helps reduce computational complexity, prevents overfitting, and introduces translational invariance to small shifts in input data.</li></ul>
<div><div><strong>Parameter Sharing</strong>:<br></div></div>	<ul><li>Definition: Technique used in convolutional layers of CNNs where the parameters (weights) of convolutional kernels are shared across different spatial locations in the input.</li><li>Benefits: Reduces the number of learnable parameters, promotes feature reuse, and enhances the generalization capability of the network.</li><li>Implementation: Achieved by constraining the weights of convolutional kernels to be identical across different locations.</li></ul>
<div>Sparse Connectivity:</div><br>	<ul><li>Definition: A property of convolutional layers in CNNs where only a subset of units in one layer are connected to units in the preceding layer.</li><li>Purpose: Reduces the computational cost and complexity of the network by focusing on relevant local features.</li><li>Implementation: Achieved by using smaller kernels compared to the input size, resulting in each output unit being influenced by a limited region of the input.</li></ul>
<div>Kernel Size Calculation:</div><br>	<ul><li>Formula: The size of the output feature map in a convolutional layer is calculated based on the size of the input feature map, the size of the kernel/filter, the stride, and the padding.</li><li>Calculation: Width = [(Width of Input + 2 * Padding - Width of Kernel) / Stride] + 1</li><li>Height: Similarly calculated using the height dimensions.</li><li>Depth: The depth (number of feature maps) remains unchanged unless explicitly altered by design.</li></ul>
<div>size of the output of convolutional layer, and parameters&nbsp;</div>	"<img src=""paste-27e04abbe24543b00ab0608a121b0f8a9a8695ee.jpg"">"
<strong>Unsupervised Learning</strong>:	<ul><li>Definition: A type of machine learning where the algorithm is trained on input data without explicit output labels.</li><li>Purpose: Used to discover patterns, clusters, or hidden structures within datasets.</li><li>Examples: Clustering, dimensionality reduction, anomaly detection.</li></ul>
<strong>K-means Algorithm</strong>:	<ul><li>Definition: An iterative clustering algorithm used to partition data into k clusters based on similarities.</li><li>Steps:<ol><li>Choose the number of clusters (k).</li><li>Initialize cluster centroids.</li><li>Assign each data point to the nearest centroid.</li><li>Update centroids based on the mean of points assigned to each cluster.</li><li>Repeat steps 3-4 until convergence.</li></ol></li><li>Limitations: Requires specifying the number of clusters beforehand, sensitive to initialization, and assumes spherical clusters.</li></ul>
<div>Gaussian Mixture Model (GMM):</div><br>	<ul><li>Definition: A probabilistic model that represents a mixture of several Gaussian distributions.</li><li>Components: Comprises k Gaussian distributions, each with its mean, covariance, and weight.</li><li>Probability Distribution: Obtained by summing the weighted Gaussian distributions.</li><li>Application: Often used in modeling complex data distributions where data points may belong to multiple clusters.</li></ul>
<strong>Expectation Maximization (EM) Algorithm</strong>:	<ul><li>Definition: An iterative optimization algorithm used to estimate the parameters of probabilistic models, such as GMMs.</li><li>Steps:<ol><li>E-step: Compute the posterior probability of each data point belonging to each cluster.</li><li>M-step: Update the parameters (mean, covariance, weight) of each Gaussian distribution based on the posterior probabilities.</li><li>Iterate between E-step and M-step until convergence.</li></ol></li><li>Considerations: More general than k-means as it estimates all parameters of the model, including covariance.</li></ul>
<strong>Convergence in EM Algorithm</strong>:	<ul><li>Challenge: EM is a greedy algorithm that may converge to local optima depending on initialization.</li><li>Initialization: The convergence behavior depends on the initial values of parameters.</li><li>Strategies: Multiple random initializations, heuristic initialization methods, or using alternative optimization techniques.</li></ul>
Clustering:	<ul><li>Definition: The process of grouping similar data points together based on their intrinsic characteristics.</li><li>Objective: To identify natural groupings or clusters within a dataset without prior knowledge of class labels.</li><li>Applications: Market segmentation, image segmentation, social network analysis.</li></ul>
<strong>Dimensionality Reduction</strong>:	<ul><li>Definition: The process of reducing the number of features or variables in a dataset while retaining the essential information.</li><li>Techniques: Principal Component Analysis (PCA), t-distributed Stochastic Neighbor Embedding (t-SNE), Autoencoders.</li><li>Benefits: Helps in visualization, reduces computational complexity, mitigates the curse of dimensionality.</li></ul>
<strong>Anomaly Detection</strong>:	<ul><li>Definition: The identification of observations that deviate significantly from the majority of the data, often indicating unusual behavior or events.</li><li>Methods: Statistical approaches, machine learning algorithms (e.g., Isolation Forest, One-Class SVM).</li><li>Use Cases: Fraud detection, network security, predictive maintenance.</li></ul>
<strong>Self-Organizing Maps (SOM)</strong>:	<ul><li>Definition: An unsupervised learning technique that maps high-dimensional data onto a low-dimensional grid, preserving the topological relationships between data points.</li><li>Structure: Consists of a grid of nodes or neurons arranged in a lattice structure.</li><li>Training: Iteratively adjusts the weights of neurons to minimize the difference between input data and weight vectors.</li></ul>
<strong>Density Estimation</strong>:	<ul><li>Definition: The process of estimating the probability density function of a dataset.</li><li>Methods: Kernel Density Estimation (KDE), Gaussian Kernel Estimation, Histogram-based methods.</li><li>Applications: Anomaly detection, generative modeling, data visualization.</li></ul>
<strong>Dimensionality Reduction</strong>:	<ul><li>Definition: The process of reducing the number of random variables under consideration while preserving the essential structure and variability of the data.</li><li>Types: Feature selection and feature extraction.</li><li>Purpose: To simplify analysis, improve computational efficiency, and enhance visualization.</li></ul>
<strong>Principal Component Analysis (PCA)</strong>:	"<ul><li>Definition: A statistical technique used for dimensionality reduction, data compression, and feature extraction.</li><li>Procedure:<ul><li>Compute the covariance matrix <span style=""font-style: italic;"">S</span> of the data.</li><li>Calculate the eigenvectors and eigenvalues of <span style=""font-style: italic;"">S</span>.</li><li>Select the eigenvectors corresponding to the largest eigenvalues (principal components).</li><li>Project the data onto the principal components to obtain a lower-dimensional representation.</li></ul></li><li>Objective: To maximize the variance of the data in the lower-dimensional space.</li><li>Formula (Projection onto principal components): Projection=<span style=""font-weight: 700;"">u</span><sub><span style=""font-style: italic;"">i</span></sub><span style=""font-style: italic;"">T</span>​<span style=""font-weight: 700;"">x</span>, where <span style=""font-weight: 700;"">u</span><sub><span style=""font-style: italic;"">i</span>​</sub> is the <span style=""font-style: italic;"">i</span>-th eigenvector.</li></ul>"
<div>Covariance Matrix:</div><br>	[latex]\begin{itemize}<br>&nbsp;&nbsp;&nbsp; \item \textbf{Definition}: A matrix that summarizes the pairwise covariances of the variables in a dataset.<br>&nbsp;&nbsp;&nbsp; \item \textbf{Formula}: $S = \frac{1}{n} \sum_{i=1}^{n} (\mathbf{x}_i - \boldsymbol{\mu})(\mathbf{x}_i - \boldsymbol{\mu})^T$, where $\boldsymbol{\mu}$ is the mean vector.<br>\end{itemize}[/latex]
<strong>Autoencoders</strong>:	<ul><li>Definition: Neural networks with hidden layers smaller in size than other layers, often used for non-linear dimensionality reduction and data compression.</li><li>Structure: Consists of an encoder and a decoder, with a bottleneck layer representing the compressed representation of the input.</li><li>Objective: To learn a compact representation of the input data in the bottleneck layer.</li></ul>
<strong>Eigenvalues and Eigenvectors</strong>:	"<ul><li>Definition: Eigenvalues represent the scaling factor of the eigenvectors in a linear transformation.</li><li>Formula: <span style=""font-style: italic;"">S</span><span style=""font-weight: 700;"">u</span>=<span style=""font-style: italic;"">λ</span><span style=""font-weight: 700;"">u</span>, where <span style=""font-style: italic;"">S</span> is the covariance matrix, <span style=""font-weight: 700;"">u</span> is the eigenvector, and <span style=""font-style: italic;"">λ</span> is the eigenvalue.</li></ul>"
<strong>Data Compression</strong>:	<ul><li>Definition: The process of reducing the amount of data needed to represent a dataset while preserving its essential information.</li><li>Techniques: PCA, autoencoders, Singular Value Decomposition (SVD).</li></ul>
<strong>Feature Selection vs. Feature Extraction</strong>:	<ul><li>Feature Selection: Selecting a subset of the original features based on their relevance to the task.</li><li>Feature Extraction: Transforming the original features into a lower-dimensional space, often using mathematical transformations or neural networks.</li></ul>
<strong>Voting:</strong>	<ul><li><strong>Description:</strong> Voting is a method of combining predictions from multiple models. Each model is trained independently on the same dataset, and their predictions are aggregated by taking a weighted average. This approach leverages the diversity among models to improve overall prediction accuracy.</li><li><strong>Example:</strong> Suppose we have three models trained to classify images. When presented with a new image, each model generates its prediction (e.g., cat, dog, or bird). The final prediction is obtained by averaging these individual predictions, giving more weight to the more reliable models.</li></ul>
How does the Mixture of Experts method dynamically adjust model contributions for better predictions?	<ul><li><strong>Description:</strong> In a mixture of experts approach, the weights used to combine predictions are determined dynamically based on the input instance. This allows different models to contribute more significantly to the final prediction in regions of the input space where they excel.</li><li><strong>Example:</strong> Consider a scenario where we have models specialized in recognizing different types of objects in images. The weight assigned to each model's prediction depends on the characteristics of the image being classified. For example, a model trained specifically to identify cars might be given more weight when analyzing images containing vehicles.</li></ul>
What's the technique that uses a meta-model to learn the best way to blend predictions from various models?	<ul><li><strong>Description:</strong> Stacking involves combining predictions from multiple models using a meta-model that learns to assign appropriate weights to each base model's output. Unlike traditional voting, the weights used in stacking are not predetermined but are learned adaptively.</li><li><strong>Example:</strong> Let's say we have several models trained to predict stock prices based on different features. Instead of simply averaging their predictions, a meta-model is trained on the outputs of these models to determine the optimal combination strategy. This allows for more nuanced adjustments to be made based on the performance of each base model.</li></ul>
How does the Cascading approach determine predictions by consulting models sequentially?	<ul><li><strong>Description:</strong> Cascading is a sequential approach to combining predictions, where models are queried one after the other. The final prediction is determined by the model with the highest confidence level, based on a predefined sequence.</li><li><strong>Example:</strong> In a cascading approach to image classification, models may be arranged in order of their reliability or specificity. When presented with an image, the first model in the sequence makes a prediction. If its confidence exceeds a certain threshold, its prediction is accepted; otherwise, the next model in line is consulted until a confident prediction is obtained.</li></ul>
What's the term for training multiple models on random subsets of data and combining their predictions?	<ul><li><strong>Description:</strong> Bagging involves training multiple models on different subsets of the dataset, obtained through random sampling with replacement. The final prediction is obtained by averaging the predictions of all models, reducing the variance and improving generalization.</li><li><strong>Example:</strong> Imagine training several decision tree models on bootstrapped samples of a dataset containing patient data. By aggregating the predictions of these models, we can obtain a more robust diagnosis for new patients, reducing the impact of individual model biases or errors.</li></ul>
What's the method that iteratively trains models, focusing on correcting errors made by previous ones?	"<ul><li><strong>Description:</strong> Boosting is a sequential learning technique where models are trained iteratively, with each subsequent model focusing on correcting the errors made by the previous ones. This approach often leads to high accuracy by emphasizing the ""hard"" examples that previous models struggled with.</li><li><strong>Example:</strong> In a boosting scenario for spam email detection, multiple weak classifiers may be trained sequentially. Each classifier focuses on identifying misclassified emails from the previous rounds, gradually improving the overall accuracy of the ensemble.</li></ul>"
What boosting algorithm adjusts sample weights to prioritize previously misclassified examples?	<ul><li><strong>Description:</strong> AdaBoost is a popular boosting algorithm where models are trained sequentially, and the weights of misclassified samples are adjusted at each iteration. This adaptive approach allows subsequent models to focus more on the examples that were previously misclassified, leading to improved overall performance.</li><li><strong>Example:</strong> Suppose we have a dataset with handwritten digit images and their corresponding labels. In AdaBoost, each weak learner may focus on a different subset of the dataset, with more emphasis placed on the samples that were misclassified by earlier models. This iterative process results in a strong ensemble model capable of accurately classifying handwritten digits.</li></ul>
<div><div>I have D-dimensional data with K components. How many parameters if I use a model with full covariance matrices? and How many if I use diaogonal covariance matrices?<br></div></div>	<div>For each Gaussian you have:<br>1. A Symmetric full&nbsp;<em>DxD</em>&nbsp;covariance matrix giving&nbsp;<code>(D*D - D)/2 + D</code>&nbsp;parameters (<code>(D*D - D)/2</code>&nbsp;is the number of off-diagonal elements and&nbsp;<code>D</code>&nbsp;is the number of diagonal elements)<br>2. A&nbsp;<em>D</em>&nbsp;dimensional mean vector giving&nbsp;<code>D</code>&nbsp;parameters<br>3. A mixing weight giving another parameter</div><div>This results in&nbsp;<code>Df = (D*D - D)/2 + 2D + 1</code>&nbsp;for each gaussian.<br>Given you have&nbsp;<em>K</em>&nbsp;components, you have&nbsp;<code>(K*Df)-1</code>&nbsp;parameters. Because the mixing weights must sum to 1, you only need to find K-1 of them. The Kth weight can be calculated by subtracting the sum of the (K-1) weights from 1.</div><div>In the diagonal case the covariance matrix parameters reduce to&nbsp;<code>D</code>, because of the abscence of off-diagonal elements.<br>Thus yielding&nbsp;<code>Df = 2D + 1</code>.</div>
